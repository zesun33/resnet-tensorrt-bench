# ResNet TensorRT Benchmark

A learning-focused project for production inference optimization with TensorRT and quantization.

## ğŸ¯ Learning Goals

By completing this project, you will understand:
- ONNX model export from PyTorch
- TensorRT engine building and optimization
- Quantization: FP32 â†’ FP16 â†’ INT8
- INT8 calibration process
- Latency vs throughput trade-offs

## âš ï¸ Prerequisites

**TensorRT SDK Required** - Install before starting:
- [NVIDIA TensorRT](https://developer.nvidia.com/tensorrt)
- Requires NVIDIA GPU with Compute Capability 6.0+

## ğŸ“š Learning Roadmap

### Phase 1: Theory (Notes)

| # | Note | Topic | Status |
|---|------|-------|--------|
| 0 | [00_tensorrt_overview.md](notes/00_tensorrt_overview.md) | TensorRT architecture and workflow | â¬œ |
| 1 | [01_quantization.md](notes/01_quantization.md) | FP32 vs FP16 vs INT8 trade-offs | â¬œ |
| 2 | [02_calibration.md](notes/02_calibration.md) | INT8 calibration process | â¬œ |
| 3 | [03_optimization_layers.md](notes/03_optimization_layers.md) | Layer fusion and optimizations | â¬œ |

### Phase 2: Model Export

| # | File | Concept | Status |
|---|------|---------|--------|
| 1 | [export_onnx.py](models/export_onnx.py) | PyTorch â†’ ONNX conversion | â¬œ |

### Phase 3: TensorRT Implementation

| # | File | Concept | Prereq Notes | Status |
|---|------|---------|--------------|--------|
| 1 | [build_engine.cpp](src/build_engine.cpp) | TensorRT engine builder | 0 | â¬œ |
| 2 | [infer_fp32.cpp](src/infer_fp32.cpp) | FP32 inference | 0 | â¬œ |
| 3 | [infer_fp16.cpp](src/infer_fp16.cpp) | FP16 inference | 1 | â¬œ |
| 4 | [infer_int8.cpp](src/infer_int8.cpp) | INT8 with calibration | 1, 2 | â¬œ |

### Phase 4: Analysis

| # | File | Purpose | Status |
|---|------|---------|--------|
| 1 | [compare_precision.py](benchmarks/compare_precision.py) | Accuracy vs latency analysis | â¬œ |

## ğŸ“ Directory Structure

```
resnet-tensorrt-bench/
â”œâ”€â”€ README.md
â”œâ”€â”€ Makefile
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ export_onnx.py           # PyTorch â†’ ONNX export
â”‚   â””â”€â”€ resnet50.onnx            # Exported model (generated)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ build_engine.cpp         # TensorRT engine builder
â”‚   â”œâ”€â”€ infer_fp32.cpp           # FP32 inference
â”‚   â”œâ”€â”€ infer_fp16.cpp           # FP16 inference
â”‚   â””â”€â”€ infer_int8.cpp           # INT8 with calibration
â”œâ”€â”€ calibration/
â”‚   â”œâ”€â”€ calibration_dataset/     # Sample images for INT8
â”‚   â””â”€â”€ int8_calibrator.cpp      # INT8 calibration class
â”œâ”€â”€ notes/
â”‚   â”œâ”€â”€ 00_tensorrt_overview.md
â”‚   â”œâ”€â”€ 01_quantization.md
â”‚   â”œâ”€â”€ 02_calibration.md
â”‚   â””â”€â”€ 03_optimization_layers.md
â””â”€â”€ benchmarks/
    â””â”€â”€ compare_precision.py
```

## ğŸ”§ Build & Run (After TensorRT Installed)

```bash
# Export model
python models/export_onnx.py

# Build TensorRT examples
make all

# Run benchmarks
./bin/infer_fp32           # FP32 baseline
./bin/infer_fp16           # FP16 (~2x faster)
./bin/infer_int8           # INT8 (~4x faster, slight accuracy loss)
```

## ğŸ“Š Expected Results

| Precision | Latency | Throughput | Accuracy |
|-----------|---------|------------|----------|
| PyTorch FP32 | ~10ms | ~100 img/s | 100% |
| TensorRT FP32 | ~5ms | ~200 img/s | 100% |
| TensorRT FP16 | ~2.5ms | ~400 img/s | ~99.9% |
| TensorRT INT8 | ~1.5ms | ~670 img/s | ~99% |

## ğŸ”— Relevance

**Micron Interview**: Covers quantization (LoRA, INT8) from job requirements. Understanding precision trade-offs is essential for memory-efficient AI.

## ğŸ“– References

- [NVIDIA TensorRT Developer Guide](https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/)
- [NVIDIA TensorRT Samples](https://github.com/NVIDIA/TensorRT)
- [Quantization Aware Training (QAT)](https://pytorch.org/docs/stable/quantization.html)
